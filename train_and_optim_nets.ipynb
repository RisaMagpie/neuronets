{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from my_models import (AlexNet, VGG16, ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ch = 3\n",
    "out_ch = 10\n",
    "\n",
    "models_list = [AlexNet, VGG16, ResNet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание папок для логов разых моделей:\n",
    "checkpoint_dir=\"./data/checkpoints/\"\n",
    "\n",
    "for name in models_list:    \n",
    "    if not os.path.isdir(checkpoint_dir+str(name)):\n",
    "        os.makedirs(checkpoint_dir+str(name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# исходники: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data/CIFAR\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(config, model_name, epoch_num=2,\n",
    "                checkpoint_dir=checkpoint_dir, data_dir=None): \n",
    "    \n",
    "    net = model_name(in_ch, out_ch)\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "\n",
    "    net.to(device)\n",
    "    \n",
    "    criterion = config[\"losses\"]() # для итерирования разных losses\n",
    "    optimizer = config[\"optimizers_names\"](net.parameters(), lr=config[\"lr\"])\n",
    "    \"\"\"\n",
    "    if checkpoint_dir: \n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir+model_name, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "    \"\"\"\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True)\n",
    "\n",
    "    for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_name, num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    load_data(data_dir)\n",
    "    \n",
    "    # заменила конфиг\n",
    "    config = {        \n",
    "        \"lr\":tune.grid_search([1e-2, 1e-1]),   \n",
    "        \"batch_size\": tune.grid_search([100, 1000]),\n",
    "        \"optimizers_names\":  tune.grid_search([optim.Adam, optim.SGD]), # ключи словаря\n",
    "        \"losses\": nn.CrossEntropyLoss \n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = JupyterNotebookReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        overwrite = True,\n",
    "        print_intermediate_tables = False,\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"precision\"]) # немного побаловалась, не нашла что еще добавить на посмотреть\n",
    "    result = tune.run(\n",
    "        partial(train_cifar, data_dir=data_dir, model_name=model_name,\n",
    "                checkpoint_dir=checkpoint_dir),\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "    local_dir= checkpoint_dir+str(model_name))\n",
    "    \n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))    \n",
    "\n",
    "    return best_trial\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 19.6/503.6 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 2.000: None | Iter 1.000: -2.3025827407836914<br>Resources requested: 8/80 CPUs, 0/8 GPUs, 0.0/338.43 GiB heap, 0.0/102.83 GiB objects (0/1.0 accelerator_type:RTX)<br>Result logdir: /notebooks/sorokina/data/checkpoints/<class 'my_models.VGG16'>/DEFAULT_2020-12-29_13-22-31<br>Number of trials: 8/8 (8 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DEFAULT_e7c4b_00006:\n",
      "  accuracy: 0.0997\n",
      "  date: 2020-12-29_13-23-22\n",
      "  done: false\n",
      "  experiment_id: c4e4fe68778149199cd1dce6c7a29c70\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.3025827407836914\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 35033\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 49.90786695480347\n",
      "  time_this_iter_s: 49.90786695480347\n",
      "  time_total_s: 49.90786695480347\n",
      "  timestamp: 1609248202\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e7c4b_00006\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-29 13:23:22,610\tINFO logger.py:721 -- Removed the following hyperparameter values when logging to tensorboard: {'optimizers_names': <class 'torch.optim.adam.Adam'>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DEFAULT_e7c4b_00003:\n",
      "  accuracy: 0.1002\n",
      "  date: 2020-12-29_13-23-22\n",
      "  done: true\n",
      "  experiment_id: 616074446e0c4be191b8956968f516a7\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.339796280860901\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 34978\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 50.39222431182861\n",
      "  time_this_iter_s: 50.39222431182861\n",
      "  time_total_s: 50.39222431182861\n",
      "  timestamp: 1609248202\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e7c4b_00003\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-29 13:23:23,008\tINFO logger.py:721 -- Removed the following hyperparameter values when logging to tensorboard: {'optimizers_names': <class 'torch.optim.adam.Adam'>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DEFAULT_e7c4b_00001:\n",
      "  accuracy: 0.0981\n",
      "  date: 2020-12-29_13-23-22\n",
      "  done: true\n",
      "  experiment_id: 574fa66eb21c470cb62ac3db8b14eba2\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.3025920391082764\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 34946\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 50.79398727416992\n",
      "  time_this_iter_s: 50.79398727416992\n",
      "  time_total_s: 50.79398727416992\n",
      "  timestamp: 1609248202\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e7c4b_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-29 13:23:23,297\tINFO logger.py:721 -- Removed the following hyperparameter values when logging to tensorboard: {'optimizers_names': <class 'torch.optim.sgd.SGD'>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DEFAULT_e7c4b_00007:\n",
      "  accuracy: 0.096\n",
      "  date: 2020-12-29_13-23-23\n",
      "  done: true\n",
      "  experiment_id: 30a16e4af9374bb780e26fc3839497ef\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.3029632806777953\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 35051\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 50.925084590911865\n",
      "  time_this_iter_s: 50.925084590911865\n",
      "  time_total_s: 50.925084590911865\n",
      "  timestamp: 1609248203\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e7c4b_00007\n",
      "  \n",
      "Result for DEFAULT_e7c4b_00002:\n",
      "  accuracy: 0.0969\n",
      "  date: 2020-12-29_13-23-23\n",
      "  done: false\n",
      "  experiment_id: 3e6ed5b6c1b64b8dba876be704070dd0\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.3025827407836914\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 34942\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 51.108625173568726\n",
      "  time_this_iter_s: 51.108625173568726\n",
      "  time_total_s: 51.108625173568726\n",
      "  timestamp: 1609248203\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e7c4b_00002\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-29 13:23:23,530\tINFO logger.py:721 -- Removed the following hyperparameter values when logging to tensorboard: {'optimizers_names': <class 'torch.optim.sgd.SGD'>}\n",
      "2020-12-29 13:23:23,591\tINFO logger.py:721 -- Removed the following hyperparameter values when logging to tensorboard: {'optimizers_names': <class 'torch.optim.sgd.SGD'>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DEFAULT_e7c4b_00004:\n",
      "  accuracy: 0.0998\n",
      "  date: 2020-12-29_13-23-23\n",
      "  done: true\n",
      "  experiment_id: bf7c363b65fe4c2bb3b4c32ef75d3578\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.303200833797455\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 34981\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 51.29124665260315\n",
      "  time_this_iter_s: 51.29124665260315\n",
      "  time_total_s: 51.29124665260315\n",
      "  timestamp: 1609248203\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e7c4b_00004\n",
      "  \n",
      "Result for DEFAULT_e7c4b_00005:\n",
      "  accuracy: 0.1033\n",
      "  date: 2020-12-29_13-23-23\n",
      "  done: true\n",
      "  experiment_id: edced6402c2846dd9738ae899ed4d99e\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.3026355028152468\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 35041\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 51.215715408325195\n",
      "  time_this_iter_s: 51.215715408325195\n",
      "  time_total_s: 51.215715408325195\n",
      "  timestamp: 1609248203\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e7c4b_00005\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model_name = VGG16\n",
    "\n",
    "best_trial = main(model_name, num_samples=1, max_num_epochs=2, gpus_per_trial=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial_conf = best_trial.config\n",
    "best_trial_conf['path']=best_trial.checkpoint.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# файлик, который говорит, где лежит чекпоинт нужной модели и какие параметры:\n",
    "\n",
    "with open(\"./data/best_trial_dir.txt\", \"w\") as file:        \n",
    "    json.dump(best_trial_conf, file,  default=lambda o: str(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
