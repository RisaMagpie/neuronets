{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUNet configs comparison: from batchflow, from batchflow with fix, mine\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "UNet configs comparison: from batchflow, from batchflow with fix, mine\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# sys.path.append('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# from tensorflow import logging\n",
    "# logging.set_verbosity(logging.ERROR)\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "from batchflow import Pipeline, B, C, V, D#, L\n",
    "from batchflow.opensets import PascalSegmentation\n",
    "from batchflow.research import Research, Option, Domain, Results, PrintLogger, RP, REP, KV\n",
    "from batchflow.models.metrics import Loss\n",
    "from batchflow.models.torch import UNet, EncoderDecoder\n",
    "# from torch.multiprocessing import set_start_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "ITERATIONS = 1#1000\n",
    "N_REPS = 1#10\n",
    "IMAGE_SHAPE = (160, 160)\n",
    "\n",
    "dataset = PascalSegmentation(bar='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_previous_results(res_name):\n",
    "    if os.path.exists(res_name):\n",
    "        shutil.rmtree(res_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 22\n",
    "\n",
    "task_config = {\n",
    "#     'inputs/images/shape': (3, 160, 160), # can be commented\n",
    "    'inputs/targets/classes': num_classes,\n",
    "#     'initial_block/inputs': 'images', # can be commented   \n",
    "    'head/layout': 'c',\n",
    "    'head/filters': num_classes,\n",
    "    'head/kernel_size': 1,\n",
    "    'loss': 'ce',\n",
    "    'optimizer': 'Adam'\n",
    "}\n",
    "\n",
    "# # UNet from batchflow\n",
    "# UNet_bf = UNet(task_config)\n",
    "\n",
    "# # fixed UNet from batchflow\n",
    "# config_bf_with_fix = dict(UNet_bf.full_config.config)\n",
    "# config_bf_with_fix['body/decoder/upsample'] = dict(layout='tna', filters=[512, 256, 128, 64])\n",
    "# UNet_bf_fix = EncoderDecoder(config_bf_with_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_UNet\n",
    "downsample_depth = 4\n",
    "\n",
    "my_config = {\n",
    "    'body/encoder': {\n",
    "        'num_stages': downsample_depth,\n",
    "        'order': ['block', 'skip', 'downsampling']\n",
    "    },    \n",
    "    'body/encoder/blocks': {\n",
    "        'layout': 'cna cna',\n",
    "        'filters': [32*pow(2, i) for i in range(1, downsample_depth+1)]\n",
    "    },\n",
    "    'body/encoder/downsample': {\n",
    "        'layout': 'p'\n",
    "    },    \n",
    "    \n",
    "    'body/embedding': {\n",
    "        'layout': 'cna cna', \n",
    "        'filters': [64*pow(2, downsample_depth), 64*pow(2, downsample_depth)]\n",
    "    },   \n",
    "\n",
    "    'body/decoder': {\n",
    "        'num_stages': downsample_depth,\n",
    "        'order': ['upsampling', 'combine', 'block']\n",
    "    },\n",
    "    'body/decoder/upsample': {\n",
    "        'layout': 'tna',\n",
    "        'filters': [32*pow(2, i) for i in range(downsample_depth, -1, -1)]\n",
    "    },\n",
    "    'body/decoder/combine': {\n",
    "        'op': 'concat'\n",
    "    },\n",
    "    'body/decoder/blocks': {\n",
    "        'layout': 'cna cna',\n",
    "        'filters': [64*pow(2, i-1) for i in range(downsample_depth, -1, -1)]\n",
    "    }\n",
    "}\n",
    "my_config.update(task_config)\n",
    "# my_UNet = EncoderDecoder(my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body/encoder': {'num_stages': 4, 'order': ['block', 'skip', 'downsampling']},\n",
       " 'body/encoder/blocks': {'layout': 'cna cna', 'filters': [64, 128, 256, 512]},\n",
       " 'body/encoder/downsample': {'layout': 'p'},\n",
       " 'body/embedding': {'layout': 'cna cna', 'filters': [1024, 1024]},\n",
       " 'body/decoder': {'num_stages': 4,\n",
       "  'order': ['upsampling', 'combine', 'block']},\n",
       " 'body/decoder/upsample': {'layout': 'tna',\n",
       "  'filters': [512, 256, 128, 64, 32]},\n",
       " 'body/decoder/combine': {'op': 'concat'},\n",
       " 'body/decoder/blocks': {'layout': 'cna cna',\n",
       "  'filters': [512, 256, 128, 64, 32.0]},\n",
       " 'inputs/targets/classes': 22,\n",
       " 'head/layout': 'c',\n",
       " 'head/filters': 22,\n",
       " 'head/kernel_size': 1,\n",
       " 'loss': 'ce',\n",
       " 'optimizer': 'Adam'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mask(x):\n",
    "    x = np.squeeze(x)\n",
    "    np.place(x, x==255, 21)\n",
    "    return x\n",
    "\n",
    "train_ppl = (dataset.train.p\n",
    "    .init_variable('loss_history', [])\n",
    "    .init_model('dynamic', C('model'), 'model', config=C('config'))\n",
    "#     .init_model('dynamic', UNet, 'model', config=task_config)\n",
    "    .resize(size=IMAGE_SHAPE, src=['images', 'labels'], dst=['images', 'labels'])\n",
    "    .to_array(channels='first', src=['images', 'labels'], dst=['images', 'labels'])\n",
    "    .process_mask(B('labels'), save_to=B('labels'))\n",
    "    .train_model('model', B('images'), B('labels'),\n",
    "                fetches='loss', save_to=V('loss_history', mode='a'))\n",
    "    .run_later(BATCH_SIZE, shuffle=True, n_epochs=None)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ppl.set_config({'config': my_config, 'model': EncoderDecoder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ppl.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ConfigAlias({'model': 'EncoderDecoder', 'config': 'my_config', 'repetition': '0'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configs = [KV(task_config, \"config_bf\"), KV(config_bf_with_fix, \"config_bf_with_fix\"), KV(my_config, \"my_config\")]\n",
    "# domain = Option('model', [UNet, UNet, EncoderDecoder]) @ Option('config', configs)\n",
    "# list(domain.iterator)\n",
    "\n",
    "configs = [KV(my_config, \"my_config\")]\n",
    "domain = Option('model', [EncoderDecoder]) @ Option('config', configs)\n",
    "list(domain.iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "research = (Research()\n",
    "            .init_domain(domain, n_reps=N_REPS)\n",
    "            .add_pipeline(train_ppl, variables='loss_history', name='train_ppl', logging=True))\n",
    "\n",
    "res_name='UNet_pascal_research'\n",
    "clear_previous_results(res_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research UNet_pascal_research is starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Domain updated: 0: : 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (2, 3, 160, 160)\n",
      "==================\n",
      "cuda:0\n",
      "<class 'numpy.ndarray'> (64, 3, 160, 160)\n",
      "==================\n",
      "cuda:0\n",
      "<class 'numpy.ndarray'> (64, 160, 160)\n",
      "==================\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Domain updated: 0: 100%|██████████| 1/1.0 [00:40<00:00, 40.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.research.research.Research at 0x7f78b4fdfd30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research.run(n_iters=ITERATIONS, name=res_name, bar=True, workers=3, devices=[2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   name          1 non-null      object\n",
      " 1   loss_history  1 non-null      object\n",
      " 2   iteration     1 non-null      int64 \n",
      " 3   sample_index  1 non-null      object\n",
      " 4   model         1 non-null      object\n",
      " 5   config        1 non-null      object\n",
      " 6   repetition    1 non-null      int64 \n",
      " 7   update        1 non-null      int64 \n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 192.0+ bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../batchflow/batchflow/research/results.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  intersection = pd.np.arange(start, end)\n",
      "../batchflow/batchflow/research/results.py:111: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  elements_to_load = pd.np.array([pd.np.isin(it, iterations_to_load) for it in iterations])\n",
      "../batchflow/batchflow/research/results.py:115: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  res[variable] = pd.np.array(dumped_file[variable])[elements_to_load]\n"
     ]
    }
   ],
   "source": [
    "results = research.load_results().df\n",
    "results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>loss_history</th>\n",
       "      <th>iteration</th>\n",
       "      <th>sample_index</th>\n",
       "      <th>model</th>\n",
       "      <th>config</th>\n",
       "      <th>repetition</th>\n",
       "      <th>update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_ppl</td>\n",
       "      <td>[3.2949722]</td>\n",
       "      <td>0</td>\n",
       "      <td>996987848</td>\n",
       "      <td>EncoderDecoder</td>\n",
       "      <td>my_config</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name loss_history  iteration sample_index           model     config  \\\n",
       "0  train_ppl  [3.2949722]          0    996987848  EncoderDecoder  my_config   \n",
       "\n",
       "   repetition  update  \n",
       "0           0       0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add test pipeline for getting metrics with param execute last"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
