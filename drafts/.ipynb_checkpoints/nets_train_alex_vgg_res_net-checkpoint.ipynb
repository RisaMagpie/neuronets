{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# исходники: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir='./data/FashionMNIST'):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.FashionMNIST(\n",
    "        root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "    testset = torchvision.datasets.FashionMNIST(\n",
    "        root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network, expand on top of nn.Module\n",
    "class AlexNet(nn.Module):\n",
    "    # сеточка с прошлого ноутбука, только убрала stride=2\n",
    "  def __init__(self, config=None):\n",
    "    super(AlexNet, self).__init__()\n",
    "\n",
    "    # define layers\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "    self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "    self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "    self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "\n",
    "  # define forward function\n",
    "  def forward(self, t):\n",
    "    # conv 1\n",
    "    t = self.conv1(t)\n",
    "    t = F.relu(t)\n",
    "    t = F.max_pool2d(t, kernel_size=2)\n",
    "\n",
    "    # conv 2\n",
    "    t = self.conv2(t)\n",
    "    t = F.relu(t)\n",
    "    t = F.max_pool2d(t, kernel_size=2)\n",
    "\n",
    "    # fc1\n",
    "    t = t.reshape(-1, 12*4*4) # x.view - оставила коммент на погуглить\n",
    "    t = self.fc1(t)\n",
    "    t = F.relu(t)\n",
    "\n",
    "    # fc2\n",
    "    t = self.fc2(t)\n",
    "    t = F.relu(t)\n",
    "\n",
    "    # output\n",
    "    t = self.out(t)\n",
    "    # don't need softmax here since we'll use cross-entropy as activation.\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(config, epoch_num=2, \n",
    "                checkpoint_dir=None, data_dir=None): \n",
    "    \n",
    "    net = AlexNet()\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "\n",
    "    net.to(device)\n",
    "    # чтобы итерировать по оптимизаторам решила ввести параметр-строку, \n",
    "    # т.к. оптимизаторы при инициализации требуют ссылку на паремтры сетки:\n",
    "    optimizers_dict={\n",
    "        \"SGD\": optim.SGD(net.parameters(), lr=config[\"lr\"]),\n",
    "        \"Adam\": optim.Adam(net.parameters(), lr=config[\"lr\"])\n",
    "    }\n",
    "    \n",
    "    criterion = config[\"losses\"] # для итерирования разных losses\n",
    "    optimizer = optimizers_dict[config[\"optimizers_names\"]]\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True)\n",
    "\n",
    "    for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, device = \"cuda:0\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    load_data(data_dir)\n",
    "    \n",
    "    # заменила конфиг, чтобы был grid_search, надо пофиксить lr, была ли ошибка из-за lr или из-за памяти - не уверена, поэтому пока оставила так\n",
    "    config = {        \n",
    "        \"lr\":1e-2,   \n",
    "        \"batch_size\": tune.grid_search([100, 1000]),\n",
    "        \"optimizers_names\":  tune.grid_search([\"Adam\", \"SGD\"]), # ключи словаря\n",
    "        \"losses\": nn.CrossEntropyLoss() # на попробовать tune.grid_search([\"\"\"nn.MultiLabelMarginLoss(), \"\"\" nn.CrossEntropyLoss()]) \n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"precision\"]) # немного побаловалась, не нашла что еще добавить на посмотреть\n",
    "    result = tune.run(\n",
    "        partial(train_cifar, data_dir=data_dir),\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "    \n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "    \n",
    "    \n",
    "    best_trained_model = AlexNet(config=best_trial.config)\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device=device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    return best_trained_model\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-28 14:08:54,356\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2020-12-28 14:08:54,361\tWARNING services.py:1640 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n",
      "2020-12-28 14:08:56,416\tWARNING experiment.py:285 -- No name detected on trainable. Using DEFAULT.\n",
      "2020-12-28 14:08:56,417\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n",
      "2020-12-28 14:08:56,482\tWARNING tune.py:409 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n",
      "2020-12-28 14:08:56,503\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.3/503.6 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 1/80 CPUs, 0/8 GPUs, 0.0/341.75 GiB heap, 0.0/103.81 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/DEFAULT_2020-12-28_14-08-56\n",
      "Number of trials: 1/4 (1 RUNNING)\n",
      "+---------------------+----------+-------+--------------+--------------------+\n",
      "| Trial name          | status   | loc   |   batch_size | optimizers_names   |\n",
      "|---------------------+----------+-------+--------------+--------------------|\n",
      "| DEFAULT_394c3_00000 | RUNNING  |       |          100 | Adam               |\n",
      "+---------------------+----------+-------+--------------+--------------------+\n",
      "\n",
      "\n",
      "Result for DEFAULT_394c3_00001:\n",
      "  accuracy: 0.7121666666666666\n",
      "  date: 2020-12-28_14-09-10\n",
      "  done: false\n",
      "  experiment_id: 6dd0e5d5ab624380be63477185995cb9\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.7337659895420074\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 7449\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 12.972107648849487\n",
      "  time_this_iter_s: 12.972107648849487\n",
      "  time_total_s: 12.972107648849487\n",
      "  timestamp: 1609164550\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 394c3_00001\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.1/503.6 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 2.000: None | Iter 1.000: -0.7337659895420074\n",
      "Resources requested: 4/80 CPUs, 0/8 GPUs, 0.0/341.75 GiB heap, 0.0/103.81 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/DEFAULT_2020-12-28_14-08-56\n",
      "Number of trials: 4/4 (4 RUNNING)\n",
      "+---------------------+----------+-----------------+--------------+--------------------+----------+------------+\n",
      "| Trial name          | status   | loc             |   batch_size | optimizers_names   |     loss |   accuracy |\n",
      "|---------------------+----------+-----------------+--------------+--------------------+----------+------------|\n",
      "| DEFAULT_394c3_00000 | RUNNING  |                 |          100 | Adam               |          |            |\n",
      "| DEFAULT_394c3_00001 | RUNNING  | 172.17.0.2:7449 |         1000 | Adam               | 0.733766 |   0.712167 |\n",
      "| DEFAULT_394c3_00002 | RUNNING  |                 |          100 | SGD                |          |            |\n",
      "| DEFAULT_394c3_00003 | RUNNING  |                 |         1000 | SGD                |          |            |\n",
      "+---------------------+----------+-----------------+--------------+--------------------+----------+------------+\n",
      "\n",
      "\n",
      "Result for DEFAULT_394c3_00003:\n",
      "  accuracy: 0.09975\n",
      "  date: 2020-12-28_14-09-10\n",
      "  done: true\n",
      "  experiment_id: a7bac191b3184bfda7671f12f27406f6\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.3026927510897317\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 7397\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 13.226828575134277\n",
      "  time_this_iter_s: 13.226828575134277\n",
      "  time_total_s: 13.226828575134277\n",
      "  timestamp: 1609164550\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 394c3_00003\n",
      "  \n",
      "Result for DEFAULT_394c3_00000:\n",
      "  accuracy: 0.8335833333333333\n",
      "  date: 2020-12-28_14-09-14\n",
      "  done: false\n",
      "  experiment_id: ce8b4f2b85244ee2b91ff82d333d2ac7\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.4604335531592369\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 7428\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 17.297733783721924\n",
      "  time_this_iter_s: 17.297733783721924\n",
      "  time_total_s: 17.297733783721924\n",
      "  timestamp: 1609164554\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 394c3_00000\n",
      "  \n",
      "Result for DEFAULT_394c3_00002:\n",
      "  accuracy: 0.17258333333333334\n",
      "  date: 2020-12-28_14-09-14\n",
      "  done: true\n",
      "  experiment_id: d67acf3b7d5c491db056943c2bd3f580\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 1\n",
      "  loss: 2.2922872344652814\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 7415\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 17.39932417869568\n",
      "  time_this_iter_s: 17.39932417869568\n",
      "  time_total_s: 17.39932417869568\n",
      "  timestamp: 1609164554\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 394c3_00002\n",
      "  \n",
      "Result for DEFAULT_394c3_00001:\n",
      "  accuracy: 0.7653333333333333\n",
      "  date: 2020-12-28_14-09-22\n",
      "  done: true\n",
      "  experiment_id: 6dd0e5d5ab624380be63477185995cb9\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.6126736005147299\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 7449\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 25.64702272415161\n",
      "  time_this_iter_s: 12.674915075302124\n",
      "  time_total_s: 25.64702272415161\n",
      "  timestamp: 1609164562\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 394c3_00001\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/503.6 GiB\n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 2.000: None | Iter 1.000: -1.5130266120036444\n",
      "Resources requested: 2/80 CPUs, 0/8 GPUs, 0.0/341.75 GiB heap, 0.0/103.81 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/DEFAULT_2020-12-28_14-08-56\n",
      "Number of trials: 4/4 (2 RUNNING, 2 TERMINATED)\n",
      "+---------------------+------------+-----------------+--------------+--------------------+----------+------------+\n",
      "| Trial name          | status     | loc             |   batch_size | optimizers_names   |     loss |   accuracy |\n",
      "|---------------------+------------+-----------------+--------------+--------------------+----------+------------|\n",
      "| DEFAULT_394c3_00000 | RUNNING    | 172.17.0.2:7428 |          100 | Adam               | 0.460434 |   0.833583 |\n",
      "| DEFAULT_394c3_00001 | RUNNING    | 172.17.0.2:7449 |         1000 | Adam               | 0.612674 |   0.765333 |\n",
      "| DEFAULT_394c3_00002 | TERMINATED |                 |          100 | SGD                | 2.29229  |   0.172583 |\n",
      "| DEFAULT_394c3_00003 | TERMINATED |                 |         1000 | SGD                | 2.30269  |   0.09975  |\n",
      "+---------------------+------------+-----------------+--------------+--------------------+----------+------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-28 14:09:31,127\tINFO tune.py:448 -- Total run time: 37.31 seconds (34.63 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DEFAULT_394c3_00000:\n",
      "  accuracy: 0.8608333333333333\n",
      "  date: 2020-12-28_14-09-31\n",
      "  done: true\n",
      "  experiment_id: ce8b4f2b85244ee2b91ff82d333d2ac7\n",
      "  hostname: 2c4060a45fe3\n",
      "  iterations_since_restore: 2\n",
      "  loss: 0.37299724568923315\n",
      "  node_ip: 172.17.0.2\n",
      "  pid: 7428\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 33.90536332130432\n",
      "  time_this_iter_s: 16.607629537582397\n",
      "  time_total_s: 33.90536332130432\n",
      "  timestamp: 1609164571\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  trial_id: 394c3_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/503.6 GiB\n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 2.000: None | Iter 1.000: -1.5130266120036444\n",
      "Resources requested: 1/80 CPUs, 0/8 GPUs, 0.0/341.75 GiB heap, 0.0/103.81 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/DEFAULT_2020-12-28_14-08-56\n",
      "Number of trials: 4/4 (1 RUNNING, 3 TERMINATED)\n",
      "+---------------------+------------+-----------------+--------------+--------------------+----------+------------+\n",
      "| Trial name          | status     | loc             |   batch_size | optimizers_names   |     loss |   accuracy |\n",
      "|---------------------+------------+-----------------+--------------+--------------------+----------+------------|\n",
      "| DEFAULT_394c3_00000 | RUNNING    | 172.17.0.2:7428 |          100 | Adam               | 0.372997 |   0.860833 |\n",
      "| DEFAULT_394c3_00001 | TERMINATED |                 |         1000 | Adam               | 0.612674 |   0.765333 |\n",
      "| DEFAULT_394c3_00002 | TERMINATED |                 |          100 | SGD                | 2.29229  |   0.172583 |\n",
      "| DEFAULT_394c3_00003 | TERMINATED |                 |         1000 | SGD                | 2.30269  |   0.09975  |\n",
      "+---------------------+------------+-----------------+--------------+--------------------+----------+------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/503.6 GiB\n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 2.000: None | Iter 1.000: -1.5130266120036444\n",
      "Resources requested: 0/80 CPUs, 0/8 GPUs, 0.0/341.75 GiB heap, 0.0/103.81 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /root/ray_results/DEFAULT_2020-12-28_14-08-56\n",
      "Number of trials: 4/4 (4 TERMINATED)\n",
      "+---------------------+------------+-------+--------------+--------------------+----------+------------+\n",
      "| Trial name          | status     | loc   |   batch_size | optimizers_names   |     loss |   accuracy |\n",
      "|---------------------+------------+-------+--------------+--------------------+----------+------------|\n",
      "| DEFAULT_394c3_00000 | TERMINATED |       |          100 | Adam               | 0.372997 |   0.860833 |\n",
      "| DEFAULT_394c3_00001 | TERMINATED |       |         1000 | Adam               | 0.612674 |   0.765333 |\n",
      "| DEFAULT_394c3_00002 | TERMINATED |       |          100 | SGD                | 2.29229  |   0.172583 |\n",
      "| DEFAULT_394c3_00003 | TERMINATED |       |         1000 | SGD                | 2.30269  |   0.09975  |\n",
      "+---------------------+------------+-------+--------------+--------------------+----------+------------+\n",
      "\n",
      "\n",
      "Best trial config: {'lr': 0.01, 'batch_size': 100, 'optimizers_names': 'Adam', 'losses': CrossEntropyLoss()}\n",
      "Best trial final validation loss: 0.37299724568923315\n",
      "Best trial final validation accuracy: 0.8608333333333333\n"
     ]
    }
   ],
   "source": [
    "best_trained_model = main(num_samples=1, max_num_epochs=2, gpus_per_trial=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26421880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26427392it [00:00, 33013786.95it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 247606.52it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4423680it [00:00, 13408048.56it/s]                           \n",
      "8192it [00:00, 98936.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpus_per_trial=1\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if gpus_per_trial > 1:\n",
    "        best_trained_model = nn.DataParallel(best_trained_model)\n",
    "\n",
    "test_acc = test_accuracy(best_trained_model, device=device)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial test set accuracy: 0.8534\n"
     ]
    }
   ],
   "source": [
    " print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
