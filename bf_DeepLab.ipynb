{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from fastai.vision.all import URLs\n",
    "from batchflow import Pipeline, B, C, D, F, V, W\n",
    "from batchflow.models.torch import EncoderDecoder\n",
    "from batchflow.models.torch.layers import ConvBlock\n",
    "from batchflow.models.torch.layers.modules import ASPP, PyramidPooling\n",
    "from batchflow.opensets import Imagenette160\n",
    "\n",
    "from train_module import training_functions\n",
    "\n",
    "GRAPH_PATH = \"./data/graphs/\"\n",
    "IMAGE_SHAPE = (3, 160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:05<00:05,  5.25s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = Imagenette160(bar='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3\n",
    "out_channels = 3\n",
    "device = \"cuda:3\"\n",
    "\n",
    "batch_size = 64\n",
    "epoch_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_depth = 2\n",
    "in_channels = 3\n",
    "config = { \n",
    "    #'device': device,\n",
    "    'inputs/images/shape': (3, 160, 160), # can be commented\n",
    "    'initial_block/inputs': 'images', # can be commented   \n",
    "    'initial_block': {\n",
    "        'layout': 'cna cna',\n",
    "        'strides': [2, 1],\n",
    "        'filters': [32, 64]\n",
    "    },\n",
    "\n",
    "    'body/encoder': {\n",
    "        'num_stages': downsample_depth,\n",
    "        'order': ['downsampling', 'skip']\n",
    "    },    \n",
    "    'body/encoder/downsample': {\n",
    "        'layout': 'R' + 'wnacna' * 3 + '|',\n",
    "        'filters': 'same',\n",
    "        'strides': [[1, 1, 1, 1, 2, 1]] * downsample_depth,\n",
    "        'kernel_size': [[3, 1] * 3] * downsample_depth,\n",
    "        'dilation_rate' : 2,\n",
    "        'branch': {\n",
    "            'layout': 'cn', \n",
    "            'kernel_size': 1,\n",
    "            'filters': 'same',\n",
    "            'strides': 2\n",
    "        },\n",
    "    },      \n",
    "\n",
    "    'body/embedding': {\n",
    "        'base' : PyramidPooling #ASPP\n",
    "    },  \n",
    "\n",
    "    'body/decoder': {\n",
    "        'num_stages': downsample_depth,\n",
    "        'order': ['block', 'combine', 'upsampling'] \n",
    "    },\n",
    "\n",
    "    'body/decoder/blocks': {\n",
    "        'layout': 'cna',\n",
    "        'kernel_size': 1,\n",
    "        'filters': 256\n",
    "    },\n",
    "\n",
    "    'body/decoder/upsample': {\n",
    "        'layout': 'b',\n",
    "        'scale_factor': 2\n",
    "    },\n",
    "    'body/decoder/combine': {\n",
    "        'op': 'concat',\n",
    "    },\n",
    "    \n",
    "    \n",
    "    'head':{\n",
    "        'layout': 'cna b',\n",
    "        'scale_factor': 2,\n",
    "        'filters': in_channels,\n",
    "        'activation': 'sigmoid'\n",
    "    },\n",
    "\n",
    "    'loss': 'mse',\n",
    "    'optimizer': 'Adam'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (initial_block): ConvBlock\n",
      "  layout=cnacna\n",
      "    Layer 0,  letter \"c\": (None, 3, 160, 160) -> (None, 32, 80, 80)\n",
      "    Layer 1,  letter \"n\": (None, 32, 80, 80) -> (None, 32, 80, 80)\n",
      "    Layer 2,  letter \"a\": (None, 32, 80, 80) -> (None, 32, 80, 80)\n",
      "    Layer 3,  letter \"c\": (None, 32, 80, 80) -> (None, 64, 80, 80)\n",
      "    Layer 4,  letter \"n\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "    Layer 5,  letter \"a\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "    \n",
      "  (body): Sequential(\n",
      "    (encoder): EncoderModule(\n",
      "      (downsample-0): ConvBlock\n",
      "      layout=Rwnacnawnacnawnacna|\n",
      "        Layer 0,    skip \"R\": (None, 64, 80, 80) -> (None, 64, 40, 40)\n",
      "        Layer 1,  letter \"w\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "        Layer 2,  letter \"n\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "        Layer 3,  letter \"a\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "        Layer 4,  letter \"c\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "        Layer 5,  letter \"n\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "        Layer 6,  letter \"a\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "        Layer 7,  letter \"w\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "        Layer 8,  letter \"n\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "        Layer 9,  letter \"a\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "        Layer 10,  letter \"c\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "        Layer 11,  letter \"n\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "        Layer 12,  letter \"a\": (None, 64, 80, 80) -> (None, 64, 80, 80)\n",
      "        Layer 13,  letter \"w\": (None, 64, 80, 80) -> (None, 64, 40, 40)\n",
      "        Layer 14,  letter \"n\": (None, 64, 40, 40) -> (None, 64, 40, 40)\n",
      "        Layer 15,  letter \"a\": (None, 64, 40, 40) -> (None, 64, 40, 40)\n",
      "        Layer 16,  letter \"c\": (None, 64, 40, 40) -> (None, 64, 40, 40)\n",
      "        Layer 17,  letter \"n\": (None, 64, 40, 40) -> (None, 64, 40, 40)\n",
      "        Layer 18,  letter \"a\": (None, 64, 40, 40) -> (None, 64, 40, 40)\n",
      "        Layer 19, combine \"|\": (None, 64, 40, 40)\n",
      "                               (None, 64, 40, 40) -> (None, 128, 40, 40)\n",
      "        \n",
      "      (skip-0): Identity()\n",
      "      (downsample-1): ConvBlock\n",
      "      layout=Rwnacnawnacnawnacna|\n",
      "        Layer 0,    skip \"R\": (None, 128, 40, 40) -> (None, 128, 20, 20)\n",
      "        Layer 1,  letter \"w\": (None, 128, 40, 40) -> (None, 128, 40, 40)\n",
      "        Layer 2,  letter \"n\": (None, 128, 40, 40) -> (None, 128, 40, 40)\n",
      "        Layer 3,  letter \"a\": (None, 128, 40, 40) -> (None, 128, 40, 40)\n",
      "        Layer 4,  letter \"c\": (None, 128, 40, 40) -> (None, 128, 40, 40)\n",
      "        Layer 5,  letter \"n\": (None, 128, 40, 40) -> (None, 128, 40, 40)\n",
      "        Layer 6,  letter \"a\": (None, 128, 40, 40) -> (None, 128, 40, 40)\n",
      "        Layer 7,  letter \"w\": (None, 128, 40, 40) -> (None, 128, 40, 40)\n",
      "        Layer 8,  letter \"n\": (None, 128, 40, 40) -> (None, 128, 40, 40)\n",
      "        Layer 9,  letter \"a\": (None, 128, 40, 40) -> (None, 128, 40, 40)\n",
      "        Layer 10,  letter \"c\": (None, 128, 40, 40) -> (None, 128, 40, 40)\n",
      "        Layer 11,  letter \"n\": (None, 128, 40, 40) -> (None, 128, 40, 40)\n",
      "        Layer 12,  letter \"a\": (None, 128, 40, 40) -> (None, 128, 40, 40)\n",
      "        Layer 13,  letter \"w\": (None, 128, 40, 40) -> (None, 128, 20, 20)\n",
      "        Layer 14,  letter \"n\": (None, 128, 20, 20) -> (None, 128, 20, 20)\n",
      "        Layer 15,  letter \"a\": (None, 128, 20, 20) -> (None, 128, 20, 20)\n",
      "        Layer 16,  letter \"c\": (None, 128, 20, 20) -> (None, 128, 20, 20)\n",
      "        Layer 17,  letter \"n\": (None, 128, 20, 20) -> (None, 128, 20, 20)\n",
      "        Layer 18,  letter \"a\": (None, 128, 20, 20) -> (None, 128, 20, 20)\n",
      "        Layer 19, combine \"|\": (None, 128, 20, 20)\n",
      "                               (None, 128, 20, 20) -> (None, 256, 20, 20)\n",
      "        \n",
      "      (skip-1): Identity()\n",
      "    )\n",
      "    (embedding): EmbeddingModule(\n",
      "      (embedding): ConvBlock\n",
      "      PyramidPooling(\n",
      "          (blocks): ModuleList(\n",
      "            (0): Identity()\n",
      "            (1): Sequential(\n",
      "              (0): ConvBlock\n",
      "              layout=pcna\n",
      "                Layer 0,  letter \"p\": (None, 256, 20, 20) -> (None, 256, 1, 1)\n",
      "                Layer 1,  letter \"c\": (None, 256, 1, 1) -> (None, 51, 1, 1)\n",
      "                Layer 2,  letter \"n\": (None, 51, 1, 1) -> (None, 51, 1, 1)\n",
      "                Layer 3,  letter \"a\": (None, 51, 1, 1) -> (None, 51, 1, 1)\n",
      "                \n",
      "              (1): Upsample(\n",
      "                (layer): ConvBlock\n",
      "                layout=b\n",
      "                  Layer 0,  letter \"b\": (None, 51, 1, 1) -> (None, 51, 20, 20)\n",
      "                  \n",
      "              )\n",
      "            )\n",
      "            (2): Sequential(\n",
      "              (0): ConvBlock\n",
      "              layout=pcna\n",
      "                Layer 0,  letter \"p\": (None, 256, 20, 20) -> (None, 256, 2, 2)\n",
      "                Layer 1,  letter \"c\": (None, 256, 2, 2) -> (None, 51, 2, 2)\n",
      "                Layer 2,  letter \"n\": (None, 51, 2, 2) -> (None, 51, 2, 2)\n",
      "                Layer 3,  letter \"a\": (None, 51, 2, 2) -> (None, 51, 2, 2)\n",
      "                \n",
      "              (1): Upsample(\n",
      "                (layer): ConvBlock\n",
      "                layout=b\n",
      "                  Layer 0,  letter \"b\": (None, 51, 2, 2) -> (None, 51, 20, 20)\n",
      "                  \n",
      "              )\n",
      "            )\n",
      "            (3): Sequential(\n",
      "              (0): ConvBlock\n",
      "              layout=pcna\n",
      "                Layer 0,  letter \"p\": (None, 256, 20, 20) -> (None, 256, 3, 3)\n",
      "                Layer 1,  letter \"c\": (None, 256, 3, 3) -> (None, 51, 3, 3)\n",
      "                Layer 2,  letter \"n\": (None, 51, 3, 3) -> (None, 51, 3, 3)\n",
      "                Layer 3,  letter \"a\": (None, 51, 3, 3) -> (None, 51, 3, 3)\n",
      "                \n",
      "              (1): Upsample(\n",
      "                (layer): ConvBlock\n",
      "                layout=b\n",
      "                  Layer 0,  letter \"b\": (None, 51, 3, 3) -> (None, 51, 20, 20)\n",
      "                  \n",
      "              )\n",
      "            )\n",
      "            (4): Sequential(\n",
      "              (0): ConvBlock\n",
      "              layout=pcna\n",
      "                Layer 0,  letter \"p\": (None, 256, 20, 20) -> (None, 256, 5, 5)\n",
      "                Layer 1,  letter \"c\": (None, 256, 5, 5) -> (None, 51, 5, 5)\n",
      "                Layer 2,  letter \"n\": (None, 51, 5, 5) -> (None, 51, 5, 5)\n",
      "                Layer 3,  letter \"a\": (None, 51, 5, 5) -> (None, 51, 5, 5)\n",
      "                \n",
      "              (1): Upsample(\n",
      "                (layer): ConvBlock\n",
      "                layout=b\n",
      "                  Layer 0,  letter \"b\": (None, 51, 5, 5) -> (None, 51, 20, 20)\n",
      "                  \n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (combine): Combine(\n",
      "            op=concat,\n",
      "            leading_idx=0,\n",
      "            input_shapes=[[(2, 256, 20, 20), (2, 51, 20, 20), (2, 51, 20, 20), (2, 51, 20, 20), (2, 51, 20, 20)]],\n",
      "            resized_shapes=[[(2, 256, 20, 20), (2, 51, 20, 20), (2, 51, 20, 20), (2, 51, 20, 20), (2, 51, 20, 20)]],\n",
      "            output_shape=(2, 460, 20, 20)\n",
      "          )\n",
      "        )\n",
      "    )\n",
      "    (decoder): DecoderModule(\n",
      "      (block-0): ConvBlock\n",
      "      DefaultBlock\n",
      "        layout=cna\n",
      "          Layer 0,  letter \"c\": (None, 460, 20, 20) -> (None, 256, 20, 20)\n",
      "          Layer 1,  letter \"n\": (None, 256, 20, 20) -> (None, 256, 20, 20)\n",
      "          Layer 2,  letter \"a\": (None, 256, 20, 20) -> (None, 256, 20, 20)\n",
      "          \n",
      "      (combine-0): Combine(\n",
      "        op=concat,\n",
      "        leading_idx=1,\n",
      "        input_shapes=[[(2, 256, 20, 20), (2, 256, 20, 20)]],\n",
      "        resized_shapes=[[(2, 256, 20, 20), (2, 256, 20, 20)]],\n",
      "        output_shape=(2, 512, 20, 20)\n",
      "      )\n",
      "      (upsample-0): Upsample(\n",
      "        (layer): ConvBlock\n",
      "        layout=b\n",
      "          Layer 0,  letter \"b\": (None, 512, 20, 20) -> (None, 512, 40, 40)\n",
      "          \n",
      "      )\n",
      "      (block-1): ConvBlock\n",
      "      DefaultBlock\n",
      "        layout=cna\n",
      "          Layer 0,  letter \"c\": (None, 512, 40, 40) -> (None, 256, 40, 40)\n",
      "          Layer 1,  letter \"n\": (None, 256, 40, 40) -> (None, 256, 40, 40)\n",
      "          Layer 2,  letter \"a\": (None, 256, 40, 40) -> (None, 256, 40, 40)\n",
      "          \n",
      "      (combine-1): Combine(\n",
      "        op=concat,\n",
      "        leading_idx=1,\n",
      "        input_shapes=[[(2, 128, 40, 40), (2, 256, 40, 40)]],\n",
      "        resized_shapes=[[(2, 128, 40, 40), (2, 256, 40, 40)]],\n",
      "        output_shape=(2, 384, 40, 40)\n",
      "      )\n",
      "      (upsample-1): Upsample(\n",
      "        (layer): ConvBlock\n",
      "        layout=b\n",
      "          Layer 0,  letter \"b\": (None, 384, 40, 40) -> (None, 384, 80, 80)\n",
      "          \n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): ConvBlock\n",
      "    layout=cnab\n",
      "      Layer 0,  letter \"c\": (None, 384, 80, 80) -> (None, 3, 80, 80)\n",
      "      Layer 1,  letter \"n\": (None, 3, 80, 80) -> (None, 3, 80, 80)\n",
      "      Layer 2,  letter \"a\": (None, 3, 80, 80) -> (None, 3, 80, 80)\n",
      "      Layer 3,  letter \"b\": (None, 3, 80, 80) -> (None, 3, 160, 160)\n",
      "      \n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.short_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (initial_block): ConvBlock(\n",
       "    (0): BaseConvBlock(\n",
       "      layout=cnacna\n",
       "      \n",
       "      (Layer 0,  letter \"c\": (None, 3, 160, 160) -> (None, 32, 80, 80)): Conv(\n",
       "        (layer): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (Layer 1,  letter \"n\": (None, 32, 80, 80) -> (None, 32, 80, 80)): BatchNorm(\n",
       "        (layer): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (Layer 2,  letter \"a\": (None, 32, 80, 80) -> (None, 32, 80, 80)): Activation(\n",
       "        (activation): ReLU(inplace=True)\n",
       "      )\n",
       "      (Layer 3,  letter \"c\": (None, 32, 80, 80) -> (None, 64, 80, 80)): Conv(\n",
       "        (layer): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (Layer 4,  letter \"n\": (None, 64, 80, 80) -> (None, 64, 80, 80)): BatchNorm(\n",
       "        (layer): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (Layer 5,  letter \"a\": (None, 64, 80, 80) -> (None, 64, 80, 80)): Activation(\n",
       "        (activation): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (body): Sequential(\n",
       "    (encoder): EncoderModule(\n",
       "      (downsample-0): ConvBlock(\n",
       "        (0): BaseConvBlock(\n",
       "          layout=Rwnacnawnacnawnacna|\n",
       "          \n",
       "          (Layer 0,    skip \"R\": (None, 64, 80, 80) -> (None, 64, 40, 40)): Branch(\n",
       "            (layer): ConvBlock(\n",
       "              (0): BaseConvBlock(\n",
       "                layout=cn\n",
       "                \n",
       "                (Layer 0,  letter \"c\": (None, 64, 80, 80) -> (None, 64, 40, 40)): Conv(\n",
       "                  (layer): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                )\n",
       "                (Layer 1,  letter \"n\": (None, 64, 40, 40) -> (None, 64, 40, 40)): BatchNorm(\n",
       "                  (layer): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (Layer 1,  letter \"w\": (None, 64, 80, 80) -> (None, 64, 80, 80)): DepthwiseConv(\n",
       "            (layer): Conv(\n",
       "              (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=64, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Layer 2,  letter \"n\": (None, 64, 80, 80) -> (None, 64, 80, 80)): BatchNorm(\n",
       "            (layer): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (Layer 3,  letter \"a\": (None, 64, 80, 80) -> (None, 64, 80, 80)): Activation(\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (Layer 4,  letter \"c\": (None, 64, 80, 80) -> (None, 64, 80, 80)): Conv(\n",
       "            (layer): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n",
       "          )\n",
       "          (Layer 5,  letter \"n\": (None, 64, 80, 80) -> (None, 64, 80, 80)): BatchNorm(\n",
       "            (layer): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (Layer 6,  letter \"a\": (None, 64, 80, 80) -> (None, 64, 80, 80)): Activation(\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (Layer 7,  letter \"w\": (None, 64, 80, 80) -> (None, 64, 80, 80)): DepthwiseConv(\n",
       "            (layer): Conv(\n",
       "              (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=64, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Layer 8,  letter \"n\": (None, 64, 80, 80) -> (None, 64, 80, 80)): BatchNorm(\n",
       "            (layer): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (Layer 9,  letter \"a\": (None, 64, 80, 80) -> (None, 64, 80, 80)): Activation(\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (Layer 10,  letter \"c\": (None, 64, 80, 80) -> (None, 64, 80, 80)): Conv(\n",
       "            (layer): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n",
       "          )\n",
       "          (Layer 11,  letter \"n\": (None, 64, 80, 80) -> (None, 64, 80, 80)): BatchNorm(\n",
       "            (layer): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (Layer 12,  letter \"a\": (None, 64, 80, 80) -> (None, 64, 80, 80)): Activation(\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (Layer 13,  letter \"w\": (None, 64, 80, 80) -> (None, 64, 40, 40)): DepthwiseConv(\n",
       "            (layer): Conv(\n",
       "              (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), dilation=(2, 2), groups=64, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Layer 14,  letter \"n\": (None, 64, 40, 40) -> (None, 64, 40, 40)): BatchNorm(\n",
       "            (layer): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (Layer 15,  letter \"a\": (None, 64, 40, 40) -> (None, 64, 40, 40)): Activation(\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (Layer 16,  letter \"c\": (None, 64, 40, 40) -> (None, 64, 40, 40)): Conv(\n",
       "            (layer): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n",
       "          )\n",
       "          (Layer 17,  letter \"n\": (None, 64, 40, 40) -> (None, 64, 40, 40)): BatchNorm(\n",
       "            (layer): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (Layer 18,  letter \"a\": (None, 64, 40, 40) -> (None, 64, 40, 40)): Activation(\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (Layer 19, combine \"|\": (None, 64, 40, 40)\n",
       "                               (None, 64, 40, 40) -> (None, 128, 40, 40)): Combine(\n",
       "            op=|,\n",
       "            leading_idx=0,\n",
       "            input_shapes=[[(2, 64, 40, 40), (2, 64, 40, 40)]],\n",
       "            resized_shapes=[[(2, 64, 40, 40), (2, 64, 40, 40)]],\n",
       "            output_shape=(2, 128, 40, 40)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (skip-0): Identity()\n",
       "      (downsample-1): ConvBlock(\n",
       "        (0): BaseConvBlock(\n",
       "          layout=Rwnacnawnacnawnacna|\n",
       "          \n",
       "          (Layer 0,    skip \"R\": (None, 128, 40, 40) -> (None, 128, 20, 20)): Branch(\n",
       "            (layer): ConvBlock(\n",
       "              (0): BaseConvBlock(\n",
       "                layout=cn\n",
       "                \n",
       "                (Layer 0,  letter \"c\": (None, 128, 40, 40) -> (None, 128, 20, 20)): Conv(\n",
       "                  (layer): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                )\n",
       "                (Layer 1,  letter \"n\": (None, 128, 20, 20) -> (None, 128, 20, 20)): BatchNorm(\n",
       "                  (layer): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (Layer 1,  letter \"w\": (None, 128, 40, 40) -> (None, 128, 40, 40)): DepthwiseConv(\n",
       "            (layer): Conv(\n",
       "              (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=128, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Layer 2,  letter \"n\": (None, 128, 40, 40) -> (None, 128, 40, 40)): BatchNorm(\n",
       "            (layer): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (Layer 3,  letter \"a\": (None, 128, 40, 40) -> (None, 128, 40, 40)): Activation(\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (Layer 4,  letter \"c\": (None, 128, 40, 40) -> (None, 128, 40, 40)): Conv(\n",
       "            (layer): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n",
       "          )\n",
       "          (Layer 5,  letter \"n\": (None, 128, 40, 40) -> (None, 128, 40, 40)): BatchNorm(\n",
       "            (layer): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (Layer 6,  letter \"a\": (None, 128, 40, 40) -> (None, 128, 40, 40)): Activation(\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (Layer 7,  letter \"w\": (None, 128, 40, 40) -> (None, 128, 40, 40)): DepthwiseConv(\n",
       "            (layer): Conv(\n",
       "              (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=128, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Layer 8,  letter \"n\": (None, 128, 40, 40) -> (None, 128, 40, 40)): BatchNorm(\n",
       "            (layer): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (Layer 9,  letter \"a\": (None, 128, 40, 40) -> (None, 128, 40, 40)): Activation(\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (Layer 10,  letter \"c\": (None, 128, 40, 40) -> (None, 128, 40, 40)): Conv(\n",
       "            (layer): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n",
       "          )\n",
       "          (Layer 11,  letter \"n\": (None, 128, 40, 40) -> (None, 128, 40, 40)): BatchNorm(\n",
       "            (layer): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (Layer 12,  letter \"a\": (None, 128, 40, 40) -> (None, 128, 40, 40)): Activation(\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (Layer 13,  letter \"w\": (None, 128, 40, 40) -> (None, 128, 20, 20)): DepthwiseConv(\n",
       "            (layer): Conv(\n",
       "              (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), dilation=(2, 2), groups=128, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (Layer 14,  letter \"n\": (None, 128, 20, 20) -> (None, 128, 20, 20)): BatchNorm(\n",
       "            (layer): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (Layer 15,  letter \"a\": (None, 128, 20, 20) -> (None, 128, 20, 20)): Activation(\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (Layer 16,  letter \"c\": (None, 128, 20, 20) -> (None, 128, 20, 20)): Conv(\n",
       "            (layer): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n",
       "          )\n",
       "          (Layer 17,  letter \"n\": (None, 128, 20, 20) -> (None, 128, 20, 20)): BatchNorm(\n",
       "            (layer): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (Layer 18,  letter \"a\": (None, 128, 20, 20) -> (None, 128, 20, 20)): Activation(\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (Layer 19, combine \"|\": (None, 128, 20, 20)\n",
       "                               (None, 128, 20, 20) -> (None, 256, 20, 20)): Combine(\n",
       "            op=|,\n",
       "            leading_idx=0,\n",
       "            input_shapes=[[(2, 128, 20, 20), (2, 128, 20, 20)]],\n",
       "            resized_shapes=[[(2, 128, 20, 20), (2, 128, 20, 20)]],\n",
       "            output_shape=(2, 256, 20, 20)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (skip-1): Identity()\n",
       "    )\n",
       "    (embedding): EmbeddingModule(\n",
       "      (embedding): ConvBlock(\n",
       "        (0): PyramidPooling(\n",
       "          (blocks): ModuleList(\n",
       "            (0): Identity()\n",
       "            (1): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (0): BaseConvBlock(\n",
       "                  layout=pcna\n",
       "                  \n",
       "                  (Layer 0,  letter \"p\": (None, 256, 20, 20) -> (None, 256, 1, 1)): Pool(\n",
       "                    (pool): MaxPool(\n",
       "                      (layer): MaxPool2d(kernel_size=(20, 20), stride=(20, 20), padding=0, dilation=1, ceil_mode=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (Layer 1,  letter \"c\": (None, 256, 1, 1) -> (None, 51, 1, 1)): Conv(\n",
       "                    (layer): Conv2d(256, 51, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  )\n",
       "                  (Layer 2,  letter \"n\": (None, 51, 1, 1) -> (None, 51, 1, 1)): BatchNorm(\n",
       "                    (layer): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (Layer 3,  letter \"a\": (None, 51, 1, 1) -> (None, 51, 1, 1)): Activation(\n",
       "                    (activation): ReLU(inplace=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(\n",
       "                (layer): ConvBlock(\n",
       "                  (0): BaseConvBlock(\n",
       "                    layout=b\n",
       "                    \n",
       "                    (Layer 0,  letter \"b\": (None, 51, 1, 1) -> (None, 51, 20, 20)): Interpolate(size=(20, 20), mode=bilinear)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (0): BaseConvBlock(\n",
       "                  layout=pcna\n",
       "                  \n",
       "                  (Layer 0,  letter \"p\": (None, 256, 20, 20) -> (None, 256, 2, 2)): Pool(\n",
       "                    (pool): MaxPool(\n",
       "                      (layer): MaxPool2d(kernel_size=(10, 10), stride=(10, 10), padding=0, dilation=1, ceil_mode=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (Layer 1,  letter \"c\": (None, 256, 2, 2) -> (None, 51, 2, 2)): Conv(\n",
       "                    (layer): Conv2d(256, 51, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  )\n",
       "                  (Layer 2,  letter \"n\": (None, 51, 2, 2) -> (None, 51, 2, 2)): BatchNorm(\n",
       "                    (layer): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (Layer 3,  letter \"a\": (None, 51, 2, 2) -> (None, 51, 2, 2)): Activation(\n",
       "                    (activation): ReLU(inplace=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(\n",
       "                (layer): ConvBlock(\n",
       "                  (0): BaseConvBlock(\n",
       "                    layout=b\n",
       "                    \n",
       "                    (Layer 0,  letter \"b\": (None, 51, 2, 2) -> (None, 51, 20, 20)): Interpolate(size=(20, 20), mode=bilinear)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (0): BaseConvBlock(\n",
       "                  layout=pcna\n",
       "                  \n",
       "                  (Layer 0,  letter \"p\": (None, 256, 20, 20) -> (None, 256, 3, 3)): Pool(\n",
       "                    (pool): MaxPool(\n",
       "                      (layer): MaxPool2d(kernel_size=(7, 7), stride=(7, 7), padding=0, dilation=1, ceil_mode=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (Layer 1,  letter \"c\": (None, 256, 3, 3) -> (None, 51, 3, 3)): Conv(\n",
       "                    (layer): Conv2d(256, 51, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  )\n",
       "                  (Layer 2,  letter \"n\": (None, 51, 3, 3) -> (None, 51, 3, 3)): BatchNorm(\n",
       "                    (layer): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (Layer 3,  letter \"a\": (None, 51, 3, 3) -> (None, 51, 3, 3)): Activation(\n",
       "                    (activation): ReLU(inplace=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(\n",
       "                (layer): ConvBlock(\n",
       "                  (0): BaseConvBlock(\n",
       "                    layout=b\n",
       "                    \n",
       "                    (Layer 0,  letter \"b\": (None, 51, 3, 3) -> (None, 51, 20, 20)): Interpolate(size=(20, 20), mode=bilinear)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (4): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (0): BaseConvBlock(\n",
       "                  layout=pcna\n",
       "                  \n",
       "                  (Layer 0,  letter \"p\": (None, 256, 20, 20) -> (None, 256, 5, 5)): Pool(\n",
       "                    (pool): MaxPool(\n",
       "                      (layer): MaxPool2d(kernel_size=(4, 4), stride=(4, 4), padding=0, dilation=1, ceil_mode=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (Layer 1,  letter \"c\": (None, 256, 5, 5) -> (None, 51, 5, 5)): Conv(\n",
       "                    (layer): Conv2d(256, 51, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  )\n",
       "                  (Layer 2,  letter \"n\": (None, 51, 5, 5) -> (None, 51, 5, 5)): BatchNorm(\n",
       "                    (layer): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (Layer 3,  letter \"a\": (None, 51, 5, 5) -> (None, 51, 5, 5)): Activation(\n",
       "                    (activation): ReLU(inplace=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): Upsample(\n",
       "                (layer): ConvBlock(\n",
       "                  (0): BaseConvBlock(\n",
       "                    layout=b\n",
       "                    \n",
       "                    (Layer 0,  letter \"b\": (None, 51, 5, 5) -> (None, 51, 20, 20)): Interpolate(size=(20, 20), mode=bilinear)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (combine): Combine(\n",
       "            op=concat,\n",
       "            leading_idx=0,\n",
       "            input_shapes=[[(2, 256, 20, 20), (2, 51, 20, 20), (2, 51, 20, 20), (2, 51, 20, 20), (2, 51, 20, 20)]],\n",
       "            resized_shapes=[[(2, 256, 20, 20), (2, 51, 20, 20), (2, 51, 20, 20), (2, 51, 20, 20), (2, 51, 20, 20)]],\n",
       "            output_shape=(2, 460, 20, 20)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): DecoderModule(\n",
       "      (block-0): ConvBlock(\n",
       "        (0): DefaultBlock(\n",
       "          (0): BaseConvBlock(\n",
       "            layout=cna\n",
       "            \n",
       "            (Layer 0,  letter \"c\": (None, 460, 20, 20) -> (None, 256, 20, 20)): Conv(\n",
       "              (layer): Conv2d(460, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (Layer 1,  letter \"n\": (None, 256, 20, 20) -> (None, 256, 20, 20)): BatchNorm(\n",
       "              (layer): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (Layer 2,  letter \"a\": (None, 256, 20, 20) -> (None, 256, 20, 20)): Activation(\n",
       "              (activation): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (combine-0): Combine(\n",
       "        op=concat,\n",
       "        leading_idx=1,\n",
       "        input_shapes=[[(2, 256, 20, 20), (2, 256, 20, 20)]],\n",
       "        resized_shapes=[[(2, 256, 20, 20), (2, 256, 20, 20)]],\n",
       "        output_shape=(2, 512, 20, 20)\n",
       "      )\n",
       "      (upsample-0): Upsample(\n",
       "        (layer): ConvBlock(\n",
       "          (0): BaseConvBlock(\n",
       "            layout=b\n",
       "            \n",
       "            (Layer 0,  letter \"b\": (None, 512, 20, 20) -> (None, 512, 40, 40)): Interpolate(scale_factor=2, mode=bilinear)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (block-1): ConvBlock(\n",
       "        (0): DefaultBlock(\n",
       "          (0): BaseConvBlock(\n",
       "            layout=cna\n",
       "            \n",
       "            (Layer 0,  letter \"c\": (None, 512, 40, 40) -> (None, 256, 40, 40)): Conv(\n",
       "              (layer): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (Layer 1,  letter \"n\": (None, 256, 40, 40) -> (None, 256, 40, 40)): BatchNorm(\n",
       "              (layer): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (Layer 2,  letter \"a\": (None, 256, 40, 40) -> (None, 256, 40, 40)): Activation(\n",
       "              (activation): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (combine-1): Combine(\n",
       "        op=concat,\n",
       "        leading_idx=1,\n",
       "        input_shapes=[[(2, 128, 40, 40), (2, 256, 40, 40)]],\n",
       "        resized_shapes=[[(2, 128, 40, 40), (2, 256, 40, 40)]],\n",
       "        output_shape=(2, 384, 40, 40)\n",
       "      )\n",
       "      (upsample-1): Upsample(\n",
       "        (layer): ConvBlock(\n",
       "          (0): BaseConvBlock(\n",
       "            layout=b\n",
       "            \n",
       "            (Layer 0,  letter \"b\": (None, 384, 40, 40) -> (None, 384, 80, 80)): Interpolate(scale_factor=2, mode=bilinear)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): ConvBlock(\n",
       "      (0): BaseConvBlock(\n",
       "        layout=cnab\n",
       "        \n",
       "        (Layer 0,  letter \"c\": (None, 384, 80, 80) -> (None, 3, 80, 80)): Conv(\n",
       "          (layer): Conv2d(384, 3, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (Layer 1,  letter \"n\": (None, 3, 80, 80) -> (None, 3, 80, 80)): BatchNorm(\n",
       "          (layer): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (Layer 2,  letter \"a\": (None, 3, 80, 80) -> (None, 3, 80, 80)): Activation(\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "        (Layer 3,  letter \"b\": (None, 3, 80, 80) -> (None, 3, 160, 160)): Interpolate(scale_factor=2, mode=bilinear)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
