{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchflow import Pipeline, D, B, V, C, R, P\n",
    "from batchflow.opensets import Imagenette160\n",
    "from batchflow.models.torch import UNet\n",
    "from batchflow import GPUMemoryMonitor\n",
    "from fastai.vision.all import URLs\n",
    "from batchflow.models.torch import EncoderDecoder\n",
    "import torch\n",
    "from train_module import training_functions\n",
    "import numpy as np\n",
    "\n",
    "import nvidia_smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mem_info(device_id):\n",
    "    nvidia_smi.nvmlInit()\n",
    "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(device_id)\n",
    "    info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "    free_memory_in_bytes = info.free\n",
    "    used_memory_in_bytes = info.used\n",
    "    nvidia_smi.nvmlShutdown()\n",
    "    return free_memory_in_bytes, used_memory_in_bytes\n",
    "\n",
    "def get_run_mem(dataset, device_id, model_config, train_pipeline, batch_size=16, n_iters=50):\n",
    "    with GPUMemoryMonitor(gpu_list=[device_id]) as monitor:\n",
    "        print(\"before and after clean\")\n",
    "        print(get_mem_info(device_id))\n",
    "        torch.cuda.empty_cache()\n",
    "        print(get_mem_info(device_id))\n",
    "        train_pipeline.run(batch_size, n_iters=n_iters, bar='n')\n",
    "    return np.max(monitor.data)\n",
    "\n",
    "def get_max_batch_size(dataset, device_id, model_config, train_pipeline, init_batch_size, n_iters):\n",
    "    print(get_mem_info(device_id))\n",
    "    first_run_memory = get_run_mem(dataset, device_id, model_config, train_pipeline, batch_size=init_batch_size, n_iters=n_iters)\n",
    "    print(get_mem_info(device_id))\n",
    "    second_run_memory = get_run_mem(dataset, device_id, model_config, train_pipeline, batch_size=2*init_batch_size, n_iters=n_iters)\n",
    "    print(get_mem_info(device_id))\n",
    "    max_batch_size = init_batch_size * (100 - 2 * first_run_memory + second_run_memory)/(second_run_memory - first_run_memory)\n",
    "    return max_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:05<00:05,  5.22s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = Imagenette160(bar=True)\n",
    "device_id = 4\n",
    "\n",
    "model_config = dict(model = UNet)\n",
    "model_config['device'] = f'cuda:{device_id}'\n",
    "model_config['loss'] = 'mse'\n",
    "\n",
    "train_pipeline = (dataset.train.p\n",
    "                .crop(shape=(160, 160), origin='center')\n",
    "                .init_variable('loss_history', [])\n",
    "                .to_array(channels='first', dtype=np.float32)\n",
    "                .multiply(1./255)\n",
    "                .init_model('dynamic', UNet, 'unet',\n",
    "                            config=model_config)\n",
    "                .train_model('unet', B.images, B.images, \n",
    "                             fetches='loss', save_to=V('loss_history', mode='a'), use_lock=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11551571968, 3145728)\n",
      "before and after clean\n",
      "(11551571968, 3145728)\n",
      "(11551571968, 3145728)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845b045879694937884f8388d1d323bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/sorokina/batchflow/batchflow/models/torch/unet.py:104: UserWarning: 'decoder/upsample/filters' are not set and can be inconsistent with 'decoder/blocks/filters'! Please revise your model's config. In future, upsample filters can be made to match decoder block's filters by default.\n",
      "  \"In future, upsample filters can be made to match decoder block's filters by default.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4613144576, 6941573120)\n",
      "before and after clean\n",
      "(4613144576, 6941573120)\n",
      "(5609291776, 5945425920)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a26fc9412044c6d8622b94a7ea594d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330760192, 11223957504)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32.1768149882904"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_batch_size = 16\n",
    "n_iters = 50\n",
    "\n",
    "print(\"Max batch size:\", get_max_batch_size(dataset, device_id, model_config, train_pipeline, init_batch_size, n_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330760192, 11223957504)\n",
      "before and after clean\n",
      "(330760192, 11223957504)\n",
      "(5609291776, 5945425920)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3e1a0bb32a417aa524afcf58657999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5607194624, 5947523072)\n",
      "before and after clean\n",
      "(5607194624, 5947523072)\n",
      "(5609291776, 5945425920)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f1ae47d78c49bd9fc8be903ab5243f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4613144576, 6941573120)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.863047235023041"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_batch_size = 8\n",
    "n_iters = 50\n",
    "\n",
    "print(\"Max batch size:\", get_max_batch_size(dataset, device_id, model_config, train_pipeline, init_batch_size, n_iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happend:\n",
    "\n",
    "***run_memory = model_size + item_size * batch_size***\n",
    "\n",
    "We set: ***init_batch_size = 16***\n",
    " \n",
    "\n",
    "So, we have two equations:\n",
    "\n",
    "***first_run_memory = model_size + init_batch_size * item_size***\n",
    "\n",
    "***second_run_memory = model_size + 2 * init_batch_size * item_size***\n",
    "\n",
    "We can get:\n",
    "\n",
    "***item_size * init_batch_size = second_run_memory - first_run_memory***\n",
    "\n",
    "***model_size = first_run_memory - item_size * init_batch_size = 2 * first_run_memory - second_run_memory***\n",
    "\n",
    "We want to know max_batch_size if we have total_memory amount of GPU memory.\n",
    "\n",
    "***max_batch_size = (total_memory - model_size)/item_size***\n",
    "\n",
    "It is equal to:\n",
    "\n",
    "***max_batch_size = (total_memory - model_size)/((second_run_memory - first_run_memory)/init_batch_size)*** \n",
    "\n",
    "where init_batch_size=16\n",
    "\n",
    "or:\n",
    "\n",
    "***max_batch_size = init_batch_size * (total_memory - model_size)/(second_run_memory - first_run_memory)*** \n",
    "\n",
    "\n",
    "Memory is measured as a percentage, so ***total_memory = 100*** %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happend with incremental multiply:\n",
    "\n",
    "***run_memory = model_size + item_size * batch_size***\n",
    "\n",
    "We set: ***init_batch_size = 16***\n",
    " \n",
    "\n",
    "So, we have two equations:\n",
    "\n",
    "***first_run_memory = model_size + (i-1) * init_batch_size * item_size***\n",
    "\n",
    "***second_run_memory = model_size + i * init_batch_size * item_size***\n",
    "\n",
    "We can get:\n",
    "\n",
    "***item_size * init_batch_size = second_run_memory - first_run_memory***\n",
    "\n",
    "***model_size = first_run_memory - (i-1) * item_size * init_batch_size = i * first_run_memory - (i-1) * second_run_memory***\n",
    "\n",
    "We want to know max_batch_size if we have total_memory amount of GPU memory.\n",
    "\n",
    "***max_batch_size = (total_memory - model_size)/item_size***\n",
    "\n",
    "It is equal to:\n",
    "\n",
    "***max_batch_size = (total_memory - model_size)/((second_run_memory - first_run_memory)/init_batch_size)*** \n",
    "\n",
    "where init_batch_size=16\n",
    "\n",
    "or:\n",
    "\n",
    "***max_batch_size = init_batch_size * (total_memory - model_size)/(second_run_memory - first_run_memory)*** \n",
    "\n",
    "\n",
    "Memory is measured as a percentage, so ***total_memory = 100*** %."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
